{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import spacy # use <!pip install spacy> and <!python -m spacy download en> if you dont have spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.optim as optim\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "#select the path where you want to save model prameters\n",
    "\n",
    "PATH = '/home/sanala/Juputer try/HSD/model-parameters/bi-lstm-hasoc.pt'\n",
    "\n",
    "\n",
    "#Select which data you want to work with \n",
    "#data path\n",
    "DataPath= '/home/sanala/Juputer try/HSD'   #change it to your path\n",
    "#data\n",
    "train_data='has21_traindata.csv'           #<has20_traindata.csv> or <has21_traindata.csv> or <has19-20-21_conmined_train.csv>\n",
    "valid_data='has21_devdata.csv'             #<has21_devdata.csv> or <has21_devdata.csv> or <has19-20-21_conmined_valid.csv>\n",
    "test_data= 'has21_testwithlabels.csv'      #<has21_testdata.csv> or <has21_testdatawithlabels.csv> <has21_testdata.csv>\n",
    "\n",
    "\n",
    "    \n",
    "def hasoc_combined_data():\n",
    "    data1a = pd.read_csv('has19_traindata.csv')\n",
    "    data2b = pd.read_csv('has19_devdata.csv')\n",
    "    data1aa = pd.read_csv('has20_traindata.csv')\n",
    "    data2bb = pd.read_csv('has20_devdata.csv')\n",
    "    data1aaa = pd.read_csv('has21_traindata.csv')\n",
    "    data2bbb = pd.read_csv('has21_devdata.csv')\n",
    "    \n",
    "    train_data, valid_data = pd.concat([data1a, data1aa, data1aaa]), pd.concat([data2b, data2bb, data2bbb])\n",
    "    test_data='has21_testwithlabels.csv'\n",
    "    #if datayear == '2020':\n",
    "            #print('Using Hasoc combined data for Hasoc 2020 test data... ')\n",
    "            #testdata = pd.read_csv(args.has20_testdata)\n",
    "    #else:\n",
    "            #print('Using Hasoc combined data for Hasoc 2021 test data... ')\n",
    "    train_data.to_csv('/home/sanala/Juputer try/HSD/has19-20-21_conmined_train.csv')\n",
    "    valid_data.to_csv('/home/sanala/Juputer try/HSD/has19-20-21_conmined_valid.csv')\n",
    "    \n",
    "\n",
    "#set to True if you want to work with hasoc_combined 19_20_21\n",
    "hasoc_combined=False\n",
    "\n",
    "if hasoc_combined:\n",
    "    hasoc_combined_data()\n",
    "    train_data='has19-20-21_conmined_train.csv'           #<has21_traindata.csv> or <has_combined_traindata.csv>\n",
    "    valid_data='has19-20-21_conmined_valid.csv'             #<has21_devdata.csv> or <has21_devdata.csv>\n",
    "    test_data= 'has21_testdatawithlabels.csv'\n",
    "    \n",
    "\n",
    "#pre-preocessing \n",
    "def text_clean(text):\n",
    "    text = re.sub(r'[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', text)                  # remove emails                    \n",
    "    text = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', text)# remove IP address\n",
    "    text = re.sub(r'http\\S+', '', text)                                          # remove URLs\n",
    "    text = re.sub(r'www\\S+ ', '', text)                                          # remove URLs\n",
    "    text = re.sub(r'[^\\w\\s#@/:%.,_-]', '', text, flags=re.UNICODE)               # remove emojis+\n",
    "    text = re.sub(r'[#,@,&,<,>,\\,/,-]', '', text)\n",
    "    #text = text.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "    text = text.replace('[','')\n",
    "    text = text.replace(']','')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace(' {2,}', ' ')                                            # remove 2 or more spaces\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d', '', text)                                              # remove numbers\n",
    "\n",
    "    return text\n",
    "\n",
    "# define model\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                 output_dim, n_layers, bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
    "                           bidirectional = bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "       \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "    \n",
    "#define function to plot training  loss vs validation loss to check  overfitting     \n",
    "def plotLosses(train_losses,val_losses):\n",
    "    plt.plot(train_losses,label='Training Loss')  \n",
    "    plt.plot(val_losses,label='Validation Loss')  \n",
    "    plt.legend() \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "#function to return  scors   \n",
    "def f1_score_func(preds, labels):\n",
    "    return f1_score(labels, preds, average=None), f1_score(labels, preds, average=\"weighted\"), f1_score(labels, preds, average=\"micro\")\n",
    "\n",
    "#training function \n",
    "def train(model, train_iterator, optimizer, criterion):\n",
    "    print ('start training' )   \n",
    "    \n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        loss = criterion(predictions, batch.task_2)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.task_2)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.item()\n",
    "        train_epoch_acc += acc.item()\n",
    "        \n",
    "    return train_epoch_loss / len(train_iterator), train_epoch_acc / len(train_iterator)\n",
    "\n",
    "#Evaluation  function\n",
    "def evaluation(model,valid_iterator):\n",
    "    print('Thin is the validation result')\n",
    "    valid_epoch_loss = 0\n",
    "    valid_epoch_acc = 0\n",
    "    predictions_tst = []\n",
    "    true_vals=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "      for batch in valid_iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            #print( predictions)\n",
    "            pred=predictions.argmax(1, keepdim = True)\n",
    "            pred = pred.transpose(1,0)\n",
    "            pred=pred.squeeze()\n",
    "            #print(pred)\n",
    "            #print(batch.task_2)\n",
    "\n",
    "            for a in pred:            # pick each element - no list comprehension\n",
    "              predictions_tst.append(a)\n",
    "            for a in batch.task_2: \n",
    "              true_vals.append(a)\n",
    "\n",
    "            loss = criterion(predictions, batch.task_2)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.task_2)\n",
    "\n",
    "            valid_epoch_loss += loss.item()\n",
    "            valid_epoch_acc += acc.item()\n",
    "    predictions_tst = torch.stack(predictions_tst)\n",
    "    true_vals = torch.stack(true_vals)\n",
    " \n",
    "       \n",
    "    return valid_epoch_loss / len(valid_iterator), valid_epoch_acc / len(valid_iterator ),predictions_tst,true_vals    \n",
    "\n",
    "#test function\n",
    "\n",
    "def test(model, test_iterator,path):\n",
    "    print ('this the testing result' )   \n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    model.eval()\n",
    "    predictions_tst = []\n",
    "    true_vals=[]\n",
    "    IDs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in test_iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            #print( predictions)\n",
    "            pred=predictions.argmax(1, keepdim = True)\n",
    "            pred = pred.transpose(1,0)\n",
    "            pred=pred.squeeze()\n",
    "            #print(pred)\n",
    "            #print(batch.task_2)\n",
    "\n",
    "            for a in pred:            # pick each element - no list comprehension\n",
    "                predictions_tst.append(a)\n",
    "            for a in batch.task_2: \n",
    "                true_vals.append(a)\n",
    "\n",
    "            for a in batch._id: \n",
    "                IDs.append(a)#better if it was a dictionary (id:label)\n",
    "            \n",
    "            loss = criterion(predictions, batch.task_2)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.task_2)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    predictions_tst = torch.stack(predictions_tst)\n",
    "    true_vals = torch.stack(true_vals)\n",
    " \n",
    "    test_loss = epoch_loss / len(test_iterator)\n",
    "    test_acc = epoch_acc / len(test_iterator)\n",
    "    return test_loss , test_acc,predictions_tst,true_vals,IDs\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "#function to test user input\n",
    "def predict_class(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    preds=preds.view(1,len(LABEL.vocab))\n",
    "    max_preds = preds.argmax(1, keepdim = True)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data='has21_traindata.csv'           #<has20_traindata.csv> or <has21_traindata.csv> or <has19-20-21_conmined_train.csv>\n",
    "#valid_data='has21_devdata.csv'             #<has21_devdata.csv> or <has21_devdata.csv> or <has19-20-21_conmined_valid.csv>\n",
    "#test_data= 'has21_testwithlabels.csv'      #<has21_testdata.csv> or <has21_testdatawithlabels.csv> <has21_testdata.csv>\n",
    "#test_data=pd.read_csv('has21_testwithlabels.csv')\n",
    "\n",
    "#test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start pre-processin\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(text_clean(s))]\n",
    "TEXT = torchtext.legacy.data.Field(tokenize = tokenizer)\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField()\n",
    "ID = torchtext.legacy.data.RawField()\n",
    "\n",
    "datafields = [('_id', ID) ,('text', TEXT),('task_1', None) ,('task_2', LABEL)]\n",
    "\n",
    "#read data\n",
    "#change the path \n",
    "trn,vld, tst = torchtext.legacy.data.TabularDataset.splits(path =DataPath, \n",
    "                                                train = train_data,\n",
    "                                                validation=valid_data,\n",
    "                                                test = test_data,    \n",
    "                                                format = 'csv',\n",
    "                                                skip_header = True,\n",
    "                                                fields = datafields)\n",
    "#check data\n",
    "#print(f'Number of training examples: {len(trn)}')\n",
    "#print(f'Number of validation examples: {len(vld)}')\n",
    "#print(f'Number of testing examples: {len(tst)}')\n",
    "\n",
    "#print(vars(trn.examples[0]))\n",
    "#print(vars(tst.examples[50]))\n",
    "\n",
    "TEXT.build_vocab(trn, max_size=25000,\n",
    "                 vectors=\"glove.6B.100d\",## #pretrained vectors are ['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d', 'glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', 'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d']\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(trn)\n",
    "#print(LABEL.vocab.stoi)\n",
    "train_iterator,valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "                                (trn,vld, tst),\n",
    "                                batch_size = 50,\n",
    "                                sort_key=lambda x: len(x.text),\n",
    "                                sort_within_batch=False,\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 20\n",
    "output_dim = len(LABEL.vocab)\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "model = RNN(input_dim, \n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            output_dim, \n",
    "            n_layers, \n",
    "            bidirectional, \n",
    "            dropout)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "#print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "model=model.to(device)\n",
    "#criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "#print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 01 | Train Loss: 1.348 | Train Acc: 34.25% | Valid Loss: 1.312 | Valid Acc: 38.71 | F1: [0.54175153 0.22068966 0.         0.05479452], weighted F1: 0.2681406275732105, micro F1: 0.3922077922077922%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.99      0.54       135\n",
      "           1       0.62      0.13      0.22       119\n",
      "           2       0.00      0.00      0.00        61\n",
      "           3       0.67      0.03      0.05        70\n",
      "\n",
      "    accuracy                           0.39       385\n",
      "   macro avg       0.41      0.29      0.20       385\n",
      "weighted avg       0.44      0.39      0.27       385\n",
      "\n",
      "Validation loss decreased (inf --> 1.311618).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 02 | Train Loss: 1.307 | Train Acc: 39.14% | Valid Loss: 1.267 | Valid Acc: 45.29 | F1: [0.55440415 0.53386454 0.         0.05555556], weighted F1: 0.3695151401362825, micro F1: 0.45714285714285713%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.79      0.55       135\n",
      "           1       0.51      0.56      0.53       119\n",
      "           2       0.00      0.00      0.00        61\n",
      "           3       1.00      0.03      0.06        70\n",
      "\n",
      "    accuracy                           0.46       385\n",
      "   macro avg       0.48      0.35      0.29       385\n",
      "weighted avg       0.49      0.46      0.37       385\n",
      "\n",
      "Validation loss decreased (1.311618 --> 1.266870).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 03 | Train Loss: 1.221 | Train Acc: 46.69% | Valid Loss: 1.211 | Valid Acc: 48.07 | F1: [0.56265985 0.6255144  0.         0.05333333], weighted F1: 0.4003340950882866, micro F1: 0.4883116883116883%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.81      0.56       135\n",
      "           1       0.61      0.64      0.63       119\n",
      "           2       0.00      0.00      0.00        61\n",
      "           3       0.40      0.03      0.05        70\n",
      "\n",
      "    accuracy                           0.49       385\n",
      "   macro avg       0.36      0.37      0.31       385\n",
      "weighted avg       0.41      0.49      0.40       385\n",
      "\n",
      "Validation loss decreased (1.266870 --> 1.210529).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 04 | Train Loss: 1.129 | Train Acc: 50.89% | Valid Loss: 1.094 | Valid Acc: 52.11 | F1: [0.55520505 0.71232877 0.1627907  0.08      ], weighted F1: 0.45519490190603473, micro F1: 0.5246753246753246%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.65      0.56       135\n",
      "           1       0.60      0.87      0.71       119\n",
      "           2       0.28      0.11      0.16        61\n",
      "           3       0.60      0.04      0.08        70\n",
      "\n",
      "    accuracy                           0.52       385\n",
      "   macro avg       0.49      0.42      0.38       385\n",
      "weighted avg       0.51      0.52      0.46       385\n",
      "\n",
      "Validation loss decreased (1.210529 --> 1.094206).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 05 | Train Loss: 1.067 | Train Acc: 54.07% | Valid Loss: 1.065 | Valid Acc: 53.43 | F1: [0.57324841 0.73170732 0.19148936 0.05333333], weighted F1: 0.4672101043135521, micro F1: 0.535064935064935%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57       135\n",
      "           1       0.62      0.88      0.73       119\n",
      "           2       0.27      0.15      0.19        61\n",
      "           3       0.40      0.03      0.05        70\n",
      "\n",
      "    accuracy                           0.54       385\n",
      "   macro avg       0.45      0.43      0.39       385\n",
      "weighted avg       0.49      0.54      0.47       385\n",
      "\n",
      "Validation loss decreased (1.094206 --> 1.065041).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 06 | Train Loss: 1.041 | Train Acc: 55.18% | Valid Loss: 1.047 | Valid Acc: 54.93 | F1: [0.58219178 0.73648649 0.26923077 0.1025641 ], weighted F1: 0.49309180884523357, micro F1: 0.5506493506493506%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.63      0.58       135\n",
      "           1       0.62      0.92      0.74       119\n",
      "           2       0.33      0.23      0.27        61\n",
      "           3       0.50      0.06      0.10        70\n",
      "\n",
      "    accuracy                           0.55       385\n",
      "   macro avg       0.50      0.46      0.42       385\n",
      "weighted avg       0.52      0.55      0.49       385\n",
      "\n",
      "Validation loss decreased (1.065041 --> 1.046800).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 07 | Train Loss: 1.003 | Train Acc: 56.83% | Valid Loss: 1.029 | Valid Acc: 57.25 | F1: [0.6137931  0.75261324 0.34188034 0.07894737], weighted F1: 0.5163741849855789, micro F1: 0.5714285714285714%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.66      0.61       135\n",
      "           1       0.64      0.91      0.75       119\n",
      "           2       0.36      0.33      0.34        61\n",
      "           3       0.50      0.04      0.08        70\n",
      "\n",
      "    accuracy                           0.57       385\n",
      "   macro avg       0.52      0.48      0.45       385\n",
      "weighted avg       0.55      0.57      0.52       385\n",
      "\n",
      "Validation loss decreased (1.046800 --> 1.028642).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 08 | Train Loss: 0.961 | Train Acc: 59.52% | Valid Loss: 1.027 | Valid Acc: 56.57 | F1: [0.62111801 0.73800738 0.28571429 0.12658228], weighted F1: 0.5141899761767357, micro F1: 0.5688311688311688%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62       135\n",
      "           1       0.66      0.84      0.74       119\n",
      "           2       0.38      0.23      0.29        61\n",
      "           3       0.56      0.07      0.13        70\n",
      "\n",
      "    accuracy                           0.57       385\n",
      "   macro avg       0.53      0.47      0.44       385\n",
      "weighted avg       0.55      0.57      0.51       385\n",
      "\n",
      "Validation loss decreased (1.028642 --> 1.026970).   Saving model ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this the testing result\n",
      "Test Loss: 0.977 | Test Acc: 60.14 F1: [0.67574468 0.75209581 0.28313253 0.1       ], weighted F1: 0.5420380007613562, micro F1: 0.600312256049961 % \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.82      0.68       483\n",
      "           1       0.69      0.83      0.75       379\n",
      "           2       0.44      0.21      0.28       224\n",
      "           3       0.44      0.06      0.10       195\n",
      "\n",
      "    accuracy                           0.60      1281\n",
      "   macro avg       0.53      0.48      0.45      1281\n",
      "weighted avg       0.56      0.60      0.54      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "valid_loss_min = np.Inf \n",
    "val_losses=[]\n",
    "train_losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc,predictions,true_vals = evaluation(model, valid_iterator)\n",
    "    val_f1, val_f1_w, val_f1_mic = f1_score_func(predictions.detach().cpu().numpy(), true_vals.detach().cpu().numpy())\n",
    "    #val_f1, val_f1_w, val_f1_mic = f1_score_func(predictions, true_vals)\n",
    "    \n",
    "    val_losses.append(valid_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f} | F1: {val_f1}, weighted F1: {val_f1_w}, micro F1: {val_f1_mic}%')\n",
    "    print(classification_report(true_vals.detach().cpu().numpy(), predictions.detach().cpu().numpy()))\n",
    "    if valid_loss <= valid_loss_min:\n",
    "      print('Validation loss decreased ({:.6f} --> {:.6f}).   Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "      valid_loss_min = valid_loss\n",
    "      torch.save(model.state_dict(), PATH)  \n",
    "plotLosses(train_losses,val_losses)\n",
    "\n",
    "test_loss , test_acc,predictions_tst,true_vals,IDs=test(model, test_iterator,PATH)\n",
    "val_f1, val_f1_w, val_f1_mic = f1_score_func(predictions_tst.detach().cpu().numpy()\n",
    ", true_vals.detach().cpu().numpy()\n",
    ")\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f} F1: {val_f1}, weighted F1: {val_f1_w}, micro F1: {val_f1_mic} % ')\n",
    "\n",
    "print(classification_report(true_vals.detach().cpu().numpy(), predictions_tst.detach().cpu().numpy()))\n",
    "\n",
    "#These are the list of the _id and needed to combine with true_vals to make the CSV for hasoc submission\n",
    "#print(IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_class = predict_class(model, \"sick I hate you mother fucker pitch\")\n",
    "#print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]} with label {pred_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-diloGPT",
   "language": "python",
   "name": "env-dilogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
