{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import spacy # use <!pip install spacy> and <!python -m spacy download en> if you dont have spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "#select the path where you want to save model prameters\n",
    "\n",
    "PATH = '/home/sanala/Juputer try/HSD/model-parameters/bi-lstm-hasoc.pt'\n",
    "\n",
    "\n",
    "#Select which data you want to work with \n",
    "#data path\n",
    "DataPath= '/home/sanala/Juputer try/HSD'   #change it to your path\n",
    "#data\n",
    "train_data='has21_traindata.csv'           #<has20_traindata.csv> or <has21_traindata.csv> or <has19-20-21_conmined_train.csv>\n",
    "valid_data='has21_devdata.csv'             #<has21_devdata.csv> or <has21_devdata.csv> or <has19-20-21_conmined_valid.csv>\n",
    "test_data= 'has21_testwithlabels.csv'            #<has21_testdata.csv> or <has21_testdatawithlabels.csv> <has21_testdata.csv>\n",
    "\n",
    "\n",
    "    \n",
    "def hasoc_combined_data():\n",
    "    data1a = pd.read_csv('has19_traindata.csv')\n",
    "    data2b = pd.read_csv('has19_devdata.csv')\n",
    "    data1aa = pd.read_csv('has20_traindata.csv')\n",
    "    data2bb = pd.read_csv('has20_devdata.csv')\n",
    "    data1aaa = pd.read_csv('has21_traindata.csv')\n",
    "    data2bbb = pd.read_csv('has21_devdata.csv')\n",
    "    \n",
    "    train_data, valid_data = pd.concat([data1a, data1aa, data1aaa]), pd.concat([data2b, data2bb, data2bbb])\n",
    "    test_data='hasoc21_testwithlabels.csv'\n",
    "    #if datayear == '2020':\n",
    "            #print('Using Hasoc combined data for Hasoc 2020 test data... ')\n",
    "            #testdata = pd.read_csv(args.has20_testdata)\n",
    "    #else:\n",
    "            #print('Using Hasoc combined data for Hasoc 2021 test data... ')\n",
    "    train_data.to_csv('/home/sanala/Juputer try/HSD/has19-20-21_conmined_train.csv')\n",
    "    valid_data.to_csv('/home/sanala/Juputer try/HSD/has19-20-21_conmined_valid.csv')\n",
    "    \n",
    "\n",
    "#set to True if you want to work with hasoc_combined 19_20_21\n",
    "hasoc_combined=False\n",
    "\n",
    "if hasoc_combined:\n",
    "    hasoc_combined_data()\n",
    "    train_data='has19-20-21_conmined_train.csv'           #<has21_traindata.csv> or <has_combined_traindata.csv>\n",
    "    valid_data='has19-20-21_conmined_valid.csv'             #<has21_devdata.csv> or <has21_devdata.csv>\n",
    "    test_data= 'has21_testdatawithlabels.csv'\n",
    "    \n",
    "\n",
    "#pre-preocessing \n",
    "def text_clean(text):\n",
    "    text = re.sub(r'[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', text)                  # remove emails                    \n",
    "    text = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', text)# remove IP address\n",
    "    text = re.sub(r'http\\S+', '', text)                                          # remove URLs\n",
    "    text = re.sub(r'www\\S+ ', '', text)                                          # remove URLs\n",
    "    text = re.sub(r'[^\\w\\s#@/:%.,_-]', '', text, flags=re.UNICODE)               # remove emojis+\n",
    "    text = re.sub(r'[#,@,&,<,>,\\,/,-]', '', text)\n",
    "    text = text.replace('[','')\n",
    "    text = text.replace(']','')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace(' {2,}', ' ')                                            # remove 2 or more spaces\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d', '', text)                                              # remove numbers\n",
    "\n",
    "    return text\n",
    "\n",
    "# define model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)\n",
    "    \n",
    "#define function to plot training  loss vs validation loss to check  overfitting     \n",
    "def plotLosses(train_losses,val_losses):\n",
    "    plt.plot(train_losses,label='Training Loss')  \n",
    "    plt.plot(val_losses,label='Validation Loss')  \n",
    "    plt.legend() \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "#function to return  scors   \n",
    "def f1_score_func(preds, labels):\n",
    "    return f1_score(labels, preds, average=None), f1_score(labels, preds, average=\"weighted\"), f1_score(labels, preds, average=\"micro\")\n",
    "\n",
    "#training function \n",
    "def train(model, train_iterator, optimizer, criterion):\n",
    "    print ('start training' )   \n",
    "    \n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        loss = criterion(predictions, batch.task_2)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.task_2)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.item()\n",
    "        train_epoch_acc += acc.item()\n",
    "        \n",
    "    return train_epoch_loss / len(train_iterator), train_epoch_acc / len(train_iterator)\n",
    "\n",
    "#Evaluation  function\n",
    "def evaluation(model,valid_iterator):\n",
    "    print('Thin is the validation result')\n",
    "    valid_epoch_loss = 0\n",
    "    valid_epoch_acc = 0\n",
    "    predictions_tst = []\n",
    "    true_vals=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "      for batch in valid_iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            #print( predictions)\n",
    "            pred=predictions.argmax(1, keepdim = True)\n",
    "            pred = pred.transpose(1,0)\n",
    "            pred=pred.squeeze()\n",
    "            #print(pred)\n",
    "            #print(batch.task_2)\n",
    "\n",
    "            for a in pred:            # pick each element - no list comprehension\n",
    "              predictions_tst.append(a)\n",
    "            for a in batch.task_2: \n",
    "              true_vals.append(a)\n",
    "\n",
    "            loss = criterion(predictions, batch.task_2)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.task_2)\n",
    "\n",
    "            valid_epoch_loss += loss.item()\n",
    "            valid_epoch_acc += acc.item()\n",
    "    predictions_tst = torch.stack(predictions_tst)\n",
    "    true_vals = torch.stack(true_vals)\n",
    " \n",
    "       \n",
    "    return valid_epoch_loss / len(valid_iterator), valid_epoch_acc / len(valid_iterator ),predictions_tst,true_vals      \n",
    "#test function\n",
    "\n",
    "def test(model, test_iterator,path):\n",
    "    print ('this the testing result' )   \n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    model.eval()\n",
    "    predictions_tst = []\n",
    "    true_vals=[]\n",
    "    IDs=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in test_iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            #print( predictions)\n",
    "            pred=predictions.argmax(1, keepdim = True)\n",
    "            pred = pred.transpose(1,0)\n",
    "            pred=pred.squeeze()\n",
    "            #print(pred)\n",
    "            #print(batch.task_2)\n",
    "\n",
    "            for a in pred:            # pick each element - no list comprehension\n",
    "                predictions_tst.append(a)\n",
    "            for a in batch.task_2: \n",
    "                true_vals.append(a)\n",
    "\n",
    "            for a in batch._id: \n",
    "                IDs.append(a)#better if it was a dictionary (id:label)\n",
    "            \n",
    "            loss = criterion(predictions, batch.task_2)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.task_2)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    predictions_tst = torch.stack(predictions_tst)\n",
    "    true_vals = torch.stack(true_vals)\n",
    " \n",
    "    test_loss = epoch_loss / len(test_iterator)\n",
    "    test_acc = epoch_acc / len(test_iterator)\n",
    "    return test_loss , test_acc,predictions_tst,true_vals,IDs\n",
    "#function to test user input\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "#function to test user input\n",
    "def predict_class(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    preds=preds.view(1,len(LABEL.vocab))\n",
    "    max_preds = preds.argmax(1, keepdim = True)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '60c5d6bf5659ea5e55defb15', 'text': ['such', 'a', 'pathetic', 'government', 'who', 'keeps', 'denying', 'that', 'there', 'is', 'no', 'shortage', 'of', 'oxygen', '....', 'shameless', 'characters', 'to', 'go', 'immediately', 'ãåëâ', 'andhbhakt', 'bjpdestroyedindia', 'ashoswai', 'sonusood', 'resignmodi'], 'task_2': 'HATE'}\n",
      "{'_id': '60c5d6bf5659ea5e55def502', 'text': ['carrie', 'isnât', 'the', 'first', 'lady', 'sheâs', 'not', 'even', 'the', 'most', 'recent', 'lady', 'that', 'was', 'a', 'violinist', '.', 'sheâs', 'the', 'âœhad', 'a', 'shag', 'whilst', 'marina', 'had', 'cancerâ', 'lady', '.'], 'task_2': 'HATE'}\n"
     ]
    }
   ],
   "source": [
    "#start pre-processin\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(text_clean(s))]\n",
    "TEXT = torchtext.legacy.data.Field(tokenize = tokenizer)\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField()\n",
    "ID = torchtext.legacy.data.RawField()\n",
    "\n",
    "datafields = [('_id', ID) ,('text', TEXT),('task_1', None) ,('task_2', LABEL)]\n",
    "\n",
    "#read data\n",
    "#change the path \n",
    "trn,vld, tst = torchtext.legacy.data.TabularDataset.splits(path =DataPath, \n",
    "                                                train = train_data,\n",
    "                                                validation=valid_data,\n",
    "                                                test = test_data,    \n",
    "                                                format = 'csv',\n",
    "                                                skip_header = True,\n",
    "                                                fields = datafields)\n",
    "#check data\n",
    "#print(f'Number of training examples: {len(trn)}')\n",
    "#print(f'Number of validation examples: {len(vld)}')\n",
    "#print(f'Number of testing examples: {len(tst)}')\n",
    "\n",
    "print(vars(trn.examples[0]))\n",
    "print(vars(tst.examples[50]))\n",
    "\n",
    "TEXT.build_vocab(trn, max_size=25000,\n",
    "                 vectors=\"glove.6B.100d\",## #pretrained vectors are ['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d', 'glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', 'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d']\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(trn)\n",
    "#print(LABEL.vocab.stoi)\n",
    "train_iterator,valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "                                (trn,vld, tst),\n",
    "                                batch_size = 50,\n",
    "                                sort_key=lambda x: len(x.text),\n",
    "                                sort_within_batch=False,\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM =len(LABEL.vocab)\n",
    "\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    " \n",
    "#print(pretrained_embeddings.shape)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM) \n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "model=model.to(device)\n",
    "#criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 01 | Train Loss: 1.213 | Train Acc: 48.68% | Valid Loss: 1.050 | Valid Acc: 56.43 | F1: [0.59689922 0.73026316 0.32380952 0.25242718], weighted F1: 0.532220766242372, micro F1: 0.5662337662337662%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.57      0.60       135\n",
      "           1       0.60      0.93      0.73       119\n",
      "           2       0.39      0.28      0.32        61\n",
      "           3       0.39      0.19      0.25        70\n",
      "\n",
      "    accuracy                           0.57       385\n",
      "   macro avg       0.50      0.49      0.48       385\n",
      "weighted avg       0.54      0.57      0.53       385\n",
      "\n",
      "Validation loss decreased (inf --> 1.049621).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 02 | Train Loss: 0.978 | Train Acc: 59.89% | Valid Loss: 1.009 | Valid Acc: 57.18 | F1: [0.60392157 0.75694444 0.42465753 0.09876543], weighted F1: 0.5309701051676611, micro F1: 0.574025974025974%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.57      0.60       135\n",
      "           1       0.64      0.92      0.76       119\n",
      "           2       0.36      0.51      0.42        61\n",
      "           3       0.36      0.06      0.10        70\n",
      "\n",
      "    accuracy                           0.57       385\n",
      "   macro avg       0.50      0.51      0.47       385\n",
      "weighted avg       0.55      0.57      0.53       385\n",
      "\n",
      "Validation loss decreased (1.049621 --> 1.008682).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 03 | Train Loss: 0.878 | Train Acc: 64.29% | Valid Loss: 0.981 | Valid Acc: 58.32 | F1: [0.63432836 0.75342466 0.2244898  0.35714286], weighted F1: 0.5558071172878044, micro F1: 0.587012987012987%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.63      0.63       135\n",
      "           1       0.64      0.92      0.75       119\n",
      "           2       0.30      0.18      0.22        61\n",
      "           3       0.48      0.29      0.36        70\n",
      "\n",
      "    accuracy                           0.59       385\n",
      "   macro avg       0.51      0.51      0.49       385\n",
      "weighted avg       0.55      0.59      0.56       385\n",
      "\n",
      "Validation loss decreased (1.008682 --> 0.981170).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 04 | Train Loss: 0.774 | Train Acc: 69.15% | Valid Loss: 0.958 | Valid Acc: 58.46 | F1: [0.63414634 0.75636364 0.33333333 0.28      ], weighted F1: 0.5598710705406935, micro F1: 0.5896103896103896%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.67      0.63       135\n",
      "           1       0.67      0.87      0.76       119\n",
      "           2       0.38      0.30      0.33        61\n",
      "           3       0.47      0.20      0.28        70\n",
      "\n",
      "    accuracy                           0.59       385\n",
      "   macro avg       0.53      0.51      0.50       385\n",
      "weighted avg       0.56      0.59      0.56       385\n",
      "\n",
      "Validation loss decreased (0.981170 --> 0.958326).   Saving model ...\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 05 | Train Loss: 0.684 | Train Acc: 74.63% | Valid Loss: 0.971 | Valid Acc: 58.82 | F1: [0.63768116 0.75618375 0.34545455 0.27722772], weighted F1: 0.5624714028906687, micro F1: 0.5922077922077922%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.65      0.64       135\n",
      "           1       0.65      0.90      0.76       119\n",
      "           2       0.39      0.31      0.35        61\n",
      "           3       0.45      0.20      0.28        70\n",
      "\n",
      "    accuracy                           0.59       385\n",
      "   macro avg       0.53      0.52      0.50       385\n",
      "weighted avg       0.56      0.59      0.56       385\n",
      "\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 06 | Train Loss: 0.593 | Train Acc: 77.56% | Valid Loss: 0.965 | Valid Acc: 60.29 | F1: [0.6539924  0.76156584 0.40944882 0.28282828], weighted F1: 0.5810121185826989, micro F1: 0.6051948051948052%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.65       135\n",
      "           1       0.66      0.90      0.76       119\n",
      "           2       0.39      0.43      0.41        61\n",
      "           3       0.48      0.20      0.28        70\n",
      "\n",
      "    accuracy                           0.61       385\n",
      "   macro avg       0.55      0.54      0.53       385\n",
      "weighted avg       0.59      0.61      0.58       385\n",
      "\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 07 | Train Loss: 0.510 | Train Acc: 81.49% | Valid Loss: 0.972 | Valid Acc: 57.21 | F1: [0.62773723 0.75       0.30088496 0.30630631], weighted F1: 0.5552985695838225, micro F1: 0.5766233766233766%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63       135\n",
      "           1       0.67      0.86      0.75       119\n",
      "           2       0.33      0.28      0.30        61\n",
      "           3       0.41      0.24      0.31        70\n",
      "\n",
      "    accuracy                           0.58       385\n",
      "   macro avg       0.51      0.50      0.50       385\n",
      "weighted avg       0.55      0.58      0.56       385\n",
      "\n",
      "start training\n",
      "Thin is the validation result\n",
      "| Epoch: 08 | Train Loss: 0.418 | Train Acc: 85.86% | Valid Loss: 0.990 | Valid Acc: 60.29 | F1: [0.66412214 0.76923077 0.3559322  0.34188034], weighted F1: 0.5891907491596973, micro F1: 0.6051948051948052%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66       135\n",
      "           1       0.68      0.88      0.77       119\n",
      "           2       0.37      0.34      0.36        61\n",
      "           3       0.43      0.29      0.34        70\n",
      "\n",
      "    accuracy                           0.61       385\n",
      "   macro avg       0.54      0.54      0.53       385\n",
      "weighted avg       0.59      0.61      0.59       385\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this the testing result\n",
      "Test Loss: 0.918 | Test Acc: 63.42 F1: [0.68477207 0.78359909 0.4076087  0.27368421], weighted F1: 0.6029677840787452, micro F1: 0.6330991412958626 % \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68       483\n",
      "           1       0.69      0.91      0.78       379\n",
      "           2       0.52      0.33      0.41       224\n",
      "           3       0.43      0.20      0.27       195\n",
      "\n",
      "    accuracy                           0.63      1281\n",
      "   macro avg       0.57      0.54      0.54      1281\n",
      "weighted avg       0.60      0.63      0.60      1281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "valid_loss_min = np.Inf \n",
    "val_losses=[]\n",
    "train_losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc,predictions,true_vals = evaluation(model, valid_iterator)\n",
    "    val_f1, val_f1_w, val_f1_mic = f1_score_func(predictions.detach().cpu().numpy(), true_vals.detach().cpu().numpy())\n",
    "    #val_f1, val_f1_w, val_f1_mic = f1_score_func(predictions, true_vals)\n",
    "    \n",
    "    val_losses.append(valid_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f} | F1: {val_f1}, weighted F1: {val_f1_w}, micro F1: {val_f1_mic}%')\n",
    "    print(classification_report(true_vals.detach().cpu().numpy(), predictions.detach().cpu().numpy()))\n",
    "    if valid_loss <= valid_loss_min:\n",
    "      print('Validation loss decreased ({:.6f} --> {:.6f}).   Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "      valid_loss_min = valid_loss\n",
    "      torch.save(model.state_dict(), PATH)  \n",
    "plotLosses(train_losses,val_losses)\n",
    "\n",
    "test_loss , test_acc,predictions_tst,true_vals,IDs=test(model, test_iterator,PATH)\n",
    "val_f1, val_f1_w, val_f1_mic = f1_score_func(predictions_tst.detach().cpu().numpy()\n",
    ", true_vals.detach().cpu().numpy()\n",
    ")\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f} F1: {val_f1}, weighted F1: {val_f1_w}, micro F1: {val_f1_mic} % ')\n",
    "\n",
    "print(classification_report(true_vals.detach().cpu().numpy(), predictions_tst.detach().cpu().numpy()))\n",
    "\n",
    "#These are the list of the _id and needed to combine with true_vals to make the CSV for hasoc submission\n",
    "#print(IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-diloGPT",
   "language": "python",
   "name": "env-dilogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
